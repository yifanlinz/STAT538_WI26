{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec83af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------\n",
    "# Data (matches your notebook)\n",
    "# -------------------------\n",
    "def sample_mog(n=5000, pi=0.4, mu1=-3.0, sigma1=1.0, mu2=3.0, sigma2=1.5, seed=538):\n",
    "    torch.manual_seed(seed)\n",
    "    z = torch.bernoulli(torch.full((n,), pi))\n",
    "    x = torch.where(\n",
    "        z == 1,\n",
    "        torch.normal(mu1, sigma1, size=(n,)),\n",
    "        torch.normal(mu2, sigma2, size=(n,))\n",
    "    )\n",
    "    return x\n",
    "\n",
    "# -------------------------\n",
    "# Generator g(eps) -> x\n",
    "# -------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=8, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z).squeeze(-1)  # (B,)\n",
    "\n",
    "# -------------------------\n",
    "# Engression loss (unconditional, 1D)\n",
    "# ES â‰ˆ mean |x - g(eps)| - 0.5 * mean |g(eps) - g(eps')|\n",
    "# We use m samples per data point in the minibatch (algorithm in notes).\n",
    "# -------------------------\n",
    "def engression_loss(x_batch, G, z_dim=8, m=8):\n",
    "    \"\"\"\n",
    "    x_batch: (B,)\n",
    "    We draw m noises per x_i: z_{i,1..m}, generate yhat_{i,1..m}.\n",
    "    term1 = mean_{i} mean_{j} |x_i - yhat_{i,j}|\n",
    "    term2 = mean_{i} mean_{j<k} |yhat_{i,j} - yhat_{i,k}|\n",
    "    loss = term1 - 0.5 * term2\n",
    "    \"\"\"\n",
    "    B = x_batch.shape[0]\n",
    "    # (B, m, z_dim)\n",
    "    z = torch.randn(B, m, z_dim, device=x_batch.device)\n",
    "    yhat = G(z.view(B * m, z_dim)).view(B, m)  # (B, m)\n",
    "\n",
    "    term1 = (yhat - x_batch[:, None]).abs().mean()\n",
    "\n",
    "    # pairwise absolute differences within each i (B, m, m)\n",
    "    diffs = (yhat[:, :, None] - yhat[:, None, :]).abs()\n",
    "    # average over j<k (avoid diagonal / double counting)\n",
    "    # mask upper triangle\n",
    "    triu_mask = torch.triu(torch.ones(m, m, device=x_batch.device), diagonal=1).bool()\n",
    "    term2 = diffs[:, triu_mask].mean()\n",
    "\n",
    "    return term1 - 0.5 * term2\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "def train_engression(x, steps=4000, batch_size=256, lr=2e-4, z_dim=8, m=8, device=\"cpu\"):\n",
    "    ds = TensorDataset(x.to(device))\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    G = Generator(z_dim=z_dim).to(device)\n",
    "    opt = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    it = 0\n",
    "    while it < steps:\n",
    "        for (xb,) in dl:\n",
    "            opt.zero_grad()\n",
    "            loss = engression_loss(xb, G, z_dim=z_dim, m=m)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            it += 1\n",
    "            if it % 500 == 0:\n",
    "                print(f\"[Engression] step={it:5d}  loss={loss.item():.4f}\")\n",
    "            if it >= steps:\n",
    "                break\n",
    "    return G\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_from_G(G, n=5000, z_dim=8, device=\"cpu\"):\n",
    "    z = torch.randn(n, z_dim, device=device)\n",
    "    return G(z).cpu()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    x = sample_mog(n=5000)  # your same mixture setup\n",
    "    G = train_engression(x, steps=4000, batch_size=256, lr=2e-4, z_dim=8, m=8, device=device)\n",
    "\n",
    "    x_fake = sample_from_G(G, n=5000, z_dim=8, device=device)\n",
    "    print(\"Generated mean/std:\", x_fake.mean().item(), x_fake.std().item())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
